
\subsection{Research Field Classification}
\label{section:field_classification}
\subsubsection{Task Description}
The goal of this task is to identify the research fields covered in social science publications.
%In general, two approaches can be applied in this task. One is the extraction of relevant terms of the publications.
%Here, relevant means best describing the research method \dd{why method?} of a publication.\dd{the "HOW" first approach is not clear at all}
%The second approach is to learn to classify publications research fields with the use of annotated data.
% Need to mention mixed approaches?
The RCC data does not provide a gold standard ---annotated training data--- for that task. To this end, we decided to train a classifier using annotated data from SSOAR.
In this way, our interpretation of the task is to select one or more labels from a given set of labels for each publication. This approach is known as a mulit-label classification. In our case, a label represents a research field.
%\textbf{Example}: \textit{P-values} (measurement) are reported for the \textit{one-tail paired t-test} (method) on \textit{Allbus} (dataset) and \textit{ISSP} (dataset).\\
%\paragraph{Formal problem definition}%\ \\[1pt]
%Let $E$ denote a set of entities. The Named Entity Recognition and Linking task consists of (i) identifying entity mentions  $m$ in a sentence and, (ii) linking them, when possible, to a  reference knowledge base  $K$ (i.e, the SAGE Thesaurus\footnote{http://methods.sagepub.com}) 
%and (iii) assigning a type to the entity, e.g., \textit{research method}, selected from a set of given types. 
%Given a textual named entity mention $m$ along with the unstructured text in which it appears, the goal is to produce a mapping from the mention  $m$ to its referent real world entity  $e$ in  $K$.

\subsubsection{Our approach - Overview }
Due to the unequal distribution of labels in the dataset, we need to guaranty enough training data for each label.
We selected only labels with frequency over 300 for training the model which results in a total of 44 labels representing research fields.
We decided to train a classification model based on the fasttext framework~\cite{joulin2017bag}. To train our model we resort to the abstracts of the publication, as this approach worked better than using the full-texts.  
%The annotated data of SSOAR contains four different annotation schemes for research field related information as described in~\ref{subsubsec:ssoar-dataset}.
%We decided to use the Classification Social Science (classoz)annotation scheme because it reflects the distinctness between research fields the best.
%Here, the strategy is to train a supervised approach\dd{you cannot "train" a unsupervised approach }. The input is a text from an article paper and the output is list of predicted research fields for the paper. Our algorithm uses title and abstract data of a paper to generate result.
%Previous researches showed that using the whole text of a paper is not good idea, since usage of texts from other parts, expands the scope of focus.\dd{need  a citation}
%Because SSOAR is a multilingual repository and the publications given by the RCC are mainly English, we need to select English training data.
%In a first step we selected all metadata records which include an English abstract and a classoz annotation.
%After this filtering 22,453 records are left over.
%These records include 156 different labels of the selected classification scheme.
%As in many classifications the number of samples per label is not equally distributed.
%Figure~\ref{figure:classsoz-label-frequence} reflects this fact for our data.



\subsubsection{Evaluation}
Figure~\ref{fig:results_fasttext} shows the performance of the model regarding various evaluation metrics for different thresholds. A label is assigned to a publication if the model outputs a probability for the label above the defined threshold. In multi-label classification, this allows us to evaluate our model from different perspectives.


%The x-a  the change of  metrics over different probability thresholds for generating the result.
\begin{comment}

This graph shows the change of different evaluation metrics over different probability thresholds for generating the result. 
The threshold defines the minimum probability of a label which is leading to an assignment.
In this experiment, only the top 3 labels with the highest probabilities were considered for the evaluations.
Orderly, micro precision and micro recall values are 0.5 for threshold 0.1
For this threshold, the model generates a prediction for all items and about half of the items have at least one correct prediction. 
All these metrics remain the same till threshold 0.2. Till threshold 0.6, we can see a dramatic increase in the micro precision and the number of items without any correct prediction. Both of these metrics pass 0.8. On the other hand, micro recall falls to the below of 0.2. In this case, selecting threshold seems hard task, since the conflict point of precision and recall is a threshold about 0.25 but both of these metrics at the point are not more than 0.3 and also we have more than 0.4 items with completely wrong predictions. Also, the default threshold doesn't look promising. In spite of micro precision about 0.7, we have a problem with the very high number of items without any prediction.
\end{comment}

\begin{figure}[t]
\centering
%\subfloat[Random Forest Evaluation]{\includegraphics[width=0.49\textwidth]{figures/research-fields/random-forest-evaluation.png}\label{fig:rf}}
\includegraphics[width=0.49\textwidth]{figures/research-fields/fast-text-evaluation.png}
%\vspace{-1em}
\caption{Precision-Recall vs. Threshold}
\label{fig:results_fasttext}
\end{figure}



%The trends of micro f1, micro recall, and publications with just wrong predictions are falling smoothly, but the trend of micro precision and number of publications without any prediction are rising gradually. 
%The Micro precision still is higher than 0.7 for threshold 0.4. Also, the number of items without prediction is lower in threshold 0.4 than threshold 0.5 (the difference is about 0.1).

% We also experiment with random forest 



%purple curve (without_prediciton) 
%The measure without_prediction describes the share of publications without any prediction at all.
%E.g. if we only accept predicted labels with a model certainty of 60\%, 40\% of the publication have no predictions. 

%red curve (share_allwrong_prediction)
%The red curve describes the share of documents in the test set, which contain only wrong predictions for each threshold.
%This means when a threshold of 20\% certainty is selected 70\% of the publications have only wrong predictions. But if a threshold of 80\% certainty is selected only 40\% of the publications have only wrong predictions. 
