---# Chapter 5 - Compettion Design**Andrew Gordon, Ekaterina Levitskaya, Jonathan Morgan, Paco Nathan andSophie Rand - New York University**We hosted a competition to address the problem of automating informationextraction from publications. The goal of the competition was to developand identify the best text analysis and machine learning techniques tofind datasets in empirical scientific publications and use thatinformation to discover relationships between data sets, researchers,publications, research methods, and fields. The results were expected tohelp to create a rich context for empirical research -- and build newmetrics to describe data use. This is an open-source project, and wesought to generate models that could be continuously improved uponthrough engagement with the research community.This paper describes how the competition was designed and discusses thelessons learned.Competition DesignThe design approach followed the successful approach developed in theNatural Language Processing (NLP) domain, which developed a series ofcompetition patterns for inspiring disparate groups of researchers tohelp to carry out information tasks against text data. These includemore basic competitions where data is provided to groups and they areallowed to train and then submit a number of runs of their modelsagainst a subset of evaluation data[^1]. We were also inspired by thedesign of the 2015 PatentsView Inventor Disambiguation TechnicalWorkshop[^2].Competition Design in PhasesThe competition had two phases. In each of the two phases, competingteams were given text and metadata for 5,000 publications and single setof metadata on 10,348 data sets of interest, shared between the twophases, for use in training and testing their models. Separate5,000-publication samples were provided for each phase. The corpus[^3]included data maintained by Deutsche Bundesbank and the set of publicdata sets hosted by the Inter-university Consortium for Political andSocial Research (ICPSR). In addition, a single 100-publicationdevelopment fold was provided separate from the training and testingdata to serve as a test for packaging of each team's model, and as aquick test of their model and the quality of its output. After the firstphase, the phase 1 holdout was also provided to phase 2 competitors toserve as additional training and testing data.All publication text provided to teams was either open access (freelyavailable) or licensed from the publisher for use in the contest. Foreach publication, participants were provided with PDF and plain textversions of each publication together with basic metadata. Copyright andlicensing around research publications limited what publications couldbe accessed, licensed, and distributed for the competition, and so ouruniverse of publications was limited to publications that were eitheropen access, or published by Sage Publications.At the end of each phase, competing teams packaged their models into adocker container. Then these containers were run on AWS by thecompetition organizers, evaluating the holdout data to generatepredictions that were used to evaluate the teams.Phase 1-------In the first phase, each publication was labeled to indicate which ofthe datasets from the master list were referenced within and whatspecific text was used to refer to each dataset. The teams used thisdata to train and tune algorithms to detect mentions of data inpublication text and, when a data set in our list is mentioned, tie eachmention to the appropriate data set.The annotated portion of the two sets of publications were drawn from aset of publications provided by Bundesbank that referenced their dataand the publications captured in the ICPSR catalog, which had beenmanually annotated as having used a particular data set for analysis.These publications were collected in a database application designed tofacilitate a mix of human and automated content analysis ofpublications. They were then filtered into two sets: those that wereopen access, and so could be shared publicly, and those that were notopen access, but that were available from our publisher partner (SagePublications, or "Sage"). Of the 5,100 total publications with annotateddata citations provided to phase 1 participants, the 2,550 publicationsin the train-test corpus (2,500) and development fold (50) were randomlyselected from the open access set, so they could be distributed freelyto all participants. The 2,500 in the holdout were randomly selectedfrom the remainder of the open access set plus those available fromSage. The un-annotated publications used in phase 1 were all publishedby Sage: the 2,550 non-annotated publications in the train-test corpus(2,500) and development fold (50) were open access publications fromSage journals. The 2,500 un-annotated publications used in the holdoutevaluation corpus were sampled from across Sage Publications' journalholdings including non-open access journals.Both the train-test publications and the holdout publications werebroken into 2,500 publications each that used one or more of the datasets of interest for analysis, as compiled by ICPSR and Bundesbankstaff, and 2,500 publications that had not been annotated and had beenfiltered to not contain data. The data set citations were captured in aseparate data set citations JSON file. The citations for the phase 1train-test publications were provided to competition teams to use astraining data, while the citations in the phase 1 holdout were used totest the quality of each team's model in phase 1, and given to teams asadditional training data in phase 2.Each team was allowed up to 2 test runs against the evaluation corpusbefore final submission. The final models of each group were evaluatedagainst the holdout corpus, along with a random qualitative review ofthe mentions, methods, and fields detected by the team's model.Submissions were primarily scored on the accuracy of techniques, thequality of documentation and code, the efficiency of the algorithm, andthe quality and novelty of the methods and research fields inferred foreach of the publications.Four finalist teams were then selected to participate in the secondphase, the teams from: Allen Institute for Artificial Intelligence,United States; GESIS at the University of Mannheim, Germany; PaderbornUniversity, Germany; and KAIST in South Korea.Phase 2-------In the second phase, finalists were provided with a new training corpusof 5000 unlabeled publications and asked to discover which of thedatasets from the first phase's data catalog were used in eachpublication, as well as infer associated research methods and fields. Asin the first phase, teams were scored on the accuracy of theirtechniques, the quality of their documentation and code, the efficiencyof their algorithm, and the quality and novelty of the methods andresearch fields inferred for each of the publications.We worked with Sage to find publications in six key topic areas ofinterest for partners and future projects (Education, Health care,Agriculture, Finance, Criminal justice, and Welfare). For 28,769matches, Sage provided PDFs for each and we parsed the text (see detailsbelow), removing any that did not parse, or that resulted in file sizessmaller than 20KB, reducing the size of the sample to 25,888. We lookedat publication year and type to see if we needed to filter out olderpublications or non-academic publications, but there were few enough ofeach class (644 pre-2000 publications and 3,115 non-research articles)that we decided we'd keep all in to preserve as much potential forheterogeneity as possible. From these 25,888 publications, we thenrandomly selected a total of 10,000 with the goal to keep thedistribution across the 6 topic areas equal (so 1666 randomly selectedin 2 topic areas, 1667 randomly selected in the other 4). Then, we splitthe phase 2 corpus to give half to participants and keep half back forevaluation, maintaining equal distribution between the topic areaswithin each set of 5,000 publications.Operational IssuesConverting PDF files--------------------Plain text provided for each publication was derived from thatpublication's PDF file by the competition organizers. It was notintended to be a gold standard, but to serve as an option in case a teampreferred not to allocate resources to PDF parsing. Articles wereconverted from PDF to text using the open source "pdftotext"application. There are multiple drawbacks with this approach, such aslosing many artifacts from PDF formatting, converting multi-columnlayouts to output text, and losing tables and chart information.Competition participants were encouraged to try their own conversionprocess if this text did not meet their needs, and if so we asked themto supply documentation so we could build a set of PDF processingstrategies to reuse in the future.Data Sets---------Competitors were provided with two sets of data related to detectingdata sets: 1) a catalog of all of the data sets of interest that modelswere tasked with finding in publications, including basic metadata forall and a list of verbatim mention text snippets for those that werecited in the train-test data; and 2) a subset of these data sets thatwere actually specifically annotated as having been used for analysis ina given publication.The data set catalog, provided to participants in the JSON filedata\_sets.json, contained metadata for all public datasets in the ICPSRdata repository and a subset of public data sets available from DeutscheBundesbank. It includes all data sets cited in the train-test andevaluation corpora, plus many others not cited in either.A major challenge with the corpus development was that ICPSR capturedwhen a given data set was used in analysis within a particularpublication, but did not capture how that determination was made. Toprovide better data for participants, we implemented a human contentanalysis protocol to capture mention text for each data set-publicationpair included in our train-test corpus. Since we manually created thisdata, given limited time and resources, we initially only did this workfor data sets that the teams would be using for training and testing inphase 1. The list of data sets cited in a particular publication is alsonot exhaustive, because the ICPSR staff only tagged datasets that wereICPSR data and used in analysis.### Data Set Mention Annotation ProcessA long term goal is to facilitate the building of generalized modelsthat are not overly dependent on the use of formal titles of data sets.We aim for models that know of and use the language of discussing andusing data to recognize where data is discussed in a particular articleand then identify which data sets. The ICPSR data contains many explicitties between publications and data sets that would have been hard tocome by otherwise, but the lack of any indication of which parts of thepublication indicated the citation relationship made it difficult toidentify the linguistic context within the publication that captured therelationship.To make it easier for participants in the competition to efficiently andsystematically engage with the language used to discuss data, wedeveloped a content analysis protocol and accompanying web-based codingapplication so human coders could examine all of the data set citationsin our train-test corpus and capture mention text for each. Thisrequired human workers to examine each data set citation in the contextof its publication (there were X citations in 2500 trainingpublications) to identify and mark locations in the text where each dataset was referenced.Because of the manual effort required, we only did this for the 2,500train-test publications that referenced data provided to the teams. Wedid not manually annotate mention text in the 2,500 publications in thephase 1 holdout, and this made that data a little less useful for teamswhen it was given to them in phase 2.Our team of coders was spread across the United States, and so we used aweb-based application with a central database store to allow ourdistributed team of coders to work in parallel. The basic unit of workwas a publication-data set pair (so a given publication would beexamined as many times as it had different data sets cited within it).The ICPSR data set repository is very fine-grained in definition of adata set, so each year of an ongoing survey, for example, might have itsown data set. To save time, we eventually created the concept of a dataset family for these types of data sets and assigned coding for any oneinstance in a family to all other instances from that family within agiven publication. So, for example, multiple years of the same survey orlongitudinal data collection were related to each other in a family, andthen coding for one year within a paper was used for all other yearscited in that paper.The general process was as follows:each user was assigned a list of citations to code.Once the user logged in to the coding tool, they were presented with alist of the coding tasks assigned to them that included a status ofeach, so they could track which they had already completed, and a linkfor each to the coding page.Once the user loads a particular citation for coding, they are presentedwith the following coding page, and are asked to follow the codinginstructions in the codebook/documentation for the annotation tool[^4]![image2.png](combined_images/chap05_figure1.png){width="6.5in"height="3.486111111111111in"}*Figure 1. Interface showing a publication and its related mentioncapturing.*Coders were instructed to find terms that relate to mentions of thedataset and avoid general synonyms of those terms (for example, tagging"ANS survey" instead of only "survey"). If the phrase providesadditional information about collection of the dataset, the mention istagged twice. For example, in the case of "ANS surveycollected/conducted by X", "ANS survey" is captured first, and then "ANSsurvey collected/conducted by X". At the same time, we tried to avoidincluding too much descriptive information of the dataset. The task isjust to code the specific mentions of a particular dataset, includingalternate names (e.g. abbreviations, etc.), rather than trying tocapture full text in which the data set is discussed.In total, a team of 5 coders, with a background in text analytics forpolicy research and computational linguistics, completed the task. Theresults were then used to re-render data\_sets.json and thedata\_set\_citations.json file for the phase 1 train-test data toinclude mentions.This combined protocol and tool were developed in-house both because oftime considerations and because some of the off-the-shelf textannotators and Qualitative Analysis tools such as lighttag.io,tag.works, NVivo, Atlas.ti, MAXQDA did not handle distributed workflows.Methods and Fields------------------For the task of detecting methods and fields for a given publication,our goals were broader than simply providing a vocabulary for each andasking the teams to classify publications against them. We want toencourage development of models that not only can figure out when agiven publication is a part of an existing field or uses an existingmethod, but that also understand enough about fields and methods suchthat they can be used to detect new fields and methods as they emerge,and can then be used to look back through time for traces of these newfields and methods to track their growth and evolution.We did not give any formal set of either methods or fields thatparticipants needed to train models to classify from. Instead, weprovided examples of taxonomies of methods and fields that SagePublications uses to classify their publications, and we directedparticipants to use them as an example, but to try to make models thatwould be more creative and potentially able to find new, emerging, ornovel fields rather than just fit a publication to a term from apredefined taxonomy. This decision to forego use of an existing taxonomyshowed the complexity of the problem of understanding fields and methodswell enough to detect them based on linguistic context. Some teamslimited themselves to the vocabularies we defined, and the results wereuninspiring. Some teams tried to detect based on text, but ended up witha lot of noise and few relevant terms.In addition, we also learned that there is complexity in "methods" thatlumping all methods together did not account for: methods could meanmany things, and we started to find sub-categories that we wish we hadbroken this into: statistical methods, analysis methods, data collectionand creation methods, etc.For future work, for each of these types of information, we intend tofirst work to decide what exactly we mean by "fields" and "methods",then find or develop one or more taxonomies to precisely capture what wemean. Once we have these taxonomies, we'll focus separately on buildingmodels to classify publications to them, and making models to extend andupdate them.Developing a submission process-------------------------------The submission process was designed to make it as straightforward andeasy as possible for a team to package their model for submission,including minimizing the understanding needed to use technologies chosenfor packaging and deployment and having a built-in way to automaticallyrun the model over the dev fold to validate processing of standard inputformats and creation of required output formats. We also wanted tominimize the installation and configuration work needed on part ofcompetition organizers to replicate computing environments as part ofmodel submission process and maximize our ability to see and be able totest how each submission environment is set up, and so avoid accepting ablackbox that could contain anything (including malicious code orsneaky/clever tricks). The git repository[^5] was integral to ourframework, but was not used directly by participants. Its coderepository was solely used as a home for the code, scripts, and filesthat made up our submission framework.Participants were instructed to work within the "project" folder intheir work folder, get their code working first on their local machine,then set up a docker container using the provided example files and getthe model running there, to isolate problems with docker from problemswith their model. Participants were allowed 2 test submissions beforethe final submission, and most groups took us up on those testsubmissions in phases 1 and 2. All groups were able to work within the"code.sh" and "project.py" files in "project" to get their model to run,so no further customizations were needed.Running a Submitted Model-------------------------Once a model was submitted, the competition organizers followed astandard script for running the model and processing its output foranalysis. Throughout this process, the evaluator communicated anyproblems with the participant team and worked with the team to addressproblems and turn around a new version of the model as quickly aspossible. If a team's model performed poorly on the standard sizemachine, we also would sometimes try different sizes of server to givethem an idea of whether their problem was related to needing morecompute power, or was a limitation of their approach independent ofavailable resources.EvaluationIn both phases of the competition, we evaluated raw mentions, researchfields, and research methods separate from citation of named data sets.Phase 1 Evaluation------------------### Mentions, Methods and FieldsIn phase 1, expert social science judges evaluated mentions, methods,and fields in two ways: 1) we randomly selected 10 publications tomanually examine each team's output against, and made notes of good andbad for each team, then ranked the teams within each publication; and 2)we generated distributions of all values found across all publicationswithin each type of value, counted the occurrences of each, compared thedistributions across teams, and ranked the teams based on how theirdistributions compared. To create overall rankings, the judges met,compared notes and individual rankings, and then agreed on an overallranking of the teams.### Data Set CitationsTo evaluate data set citations in phase 1, we used the ICPSR citationdata as our evaluation baseline for creating a confusion matrix based onhow each team's citation findings compared to ICPSR's baseline, and wecalculated precision, recall, and F1 scores from the confusion matrix tocompare across teams. To create the confusion matrix for each team, westarted with a list of all of the data set-publication pairs foundeither in ICPSR's baseline or the team's output. We created found-or-not(1 or 0) vectors for every publication-data set pair for the baseline,and for the team. Then, for each data set-publication pair, we comparedthe values between the baseline vector and the team vector to decide howto update the confusion matrix for that pair: if a team agreed withICPSR on presence of a data set, that was counted as a true positive(TP). If the team found a data set that ICPSR did not, that was countedas a false positive (FP). If a team missed a data set ICPSR indicatedwas present, it was counted as a false negative (FN). We did not developa way to capture true negatives since the metrics we used to evaluatedid not require it. In addition, as part of the processing to create theoverall confusion matrix, we created per-publication confusion matricesfor each publication, so we could track average false positives andfalse negatives per publication, and highlight publications that werehigher than the average, for more detailed evaluation.We also deferred figuring out "mentioned" vs. "used in analysis" in ourinitial competition, to make the initial task more manageable. Thisdecision, combined with the traits of the ICPSR data, caused substantialnoise in the phase 1 precision/recall/F1 scores. For example, evenmodels that figured out that a longitudinal data set was presentsometimes got many false positives and false negatives because they gotthe years wrong, and models that correctly found ICPSR data sets used indiscussion had those counted as false positives because ICPSR had onlycaptured data sets used in analysis.Phase 2 Evaluation------------------### Mentions, Methods and FieldsIn evaluating phase 2, we kept the division between mentions, fields,and methods and citations, but we refined our evaluation methods basedon what we'd learned in the first phase. We kept the basic strategy of:1) comparing the values created by each team's model in the context of aset of selected publications and 2) reviewing the overall distributionsof values for each team.We expanded the number of publications across which we compared valuesto make the sample reviewed more representative, though, and created aweb-based tool to help judges deal with the added work from morepublications to review. We also selected publications differently fordata mentions from fields and methods, choosing publications withdifferent levels of agreement between the teams on whether data waspresent or not, to start to evaluate the different model's ability todetect data at all, in addition to comparing the results when theythought a publication contained data.For fields and methods (and data set citations), we selected 20publications for each of our 6 topic areas of interest (Education,Health care, Agriculture, Finance, Criminal justice, and Welfare) with afew extras (2 extra in finance and 1 extra in criminal justice), for atotal of 123 publications to compare values across. Within the 20publications per topic area, we worked through a random selection ofarticles picking publications to add to our sample to fill out a roughratio within each topic area of 5:4:1 between publications with titleddata sets (5); data described, but not titled (4); and no data (1).To make it easier for the judges to work through this increased numberof publications, we also created a tool that collected the output foreach team side-by-side per publication along with a link to eachpublication's PDF, and had a place for the judge to score each team'soutput for a given publication from among "--1", "0", and "1". Oncejudges scored all output, we then created rankings based on the sum ofeach team's scores.![image1.png](combined_images/chap05_figure2.png){width="6.5in" height="5.0in"}For manual evaluation of data set mentions, we used the same tooldescribed above, but we chose a different sample of 60 publicationsbased on agreement between the output of the different participant teammodels as to whether publications had data mentions. We then asked aseparate pair of qualitative judges to use the tool to compare andevaluate the data set mentions generated by the teams across thesepublications.### Data Set CitationsOur analysis of data set citations in phase 2 required a moresubstantial rethinking since we did not have any starting point forpresence or absence of data like the ICPSR corpus. We implemented amethod of creating a confusion matrix that could be used to generateprecision, recall, and F1 scores more closely aligned with the task we'dassigned the teams to implement - finding mentions of data and data setswithin publications.To implement this, we started with the sample of 123 publications usedfor evaluating mentions and fields above and:Captured all "data references" within each of those publications using anew human coding protocol. This included external titled data setseither discussed or used in analysis, external data without a title thatwas discussed or used in analysis, and data created by the researcherfor a given study.For each data reference, we compared all mentions and citations createdby each team for the publication to the information on the datareference within that publication and marked any that were "related" tothe data reference.Finally, we used the list of references as a baseline and built aconfusion matrix based on whether each team had found mentions orcitations "related" to each of the data references, along with a "falsepositive" record where the baseline was always 0 and the team wasassigned a 1 if they had one or more mentions or citations that were not"related" to any data reference.To capture data references in our sample of publications, we created abasic protocol for an initial round of data creation then evaluated theresults throughout the rest of the process. We used a single datareference coder to encourage consistency in output. We tried to capturedetailed context on each reference in order to make it easier forreviewers of this data to evaluate the quality of each data referenceand to give more context for judges deciding if mentions and citationsfor a given team were "related" to a given data reference.After the data references were captured, a team of coders then looked ateach data reference related to the selected publications for each teamto see if data set citations and mentions by the team were "related" tothe data reference. The coders, subject matter experts in the differentkey topic areas, looked at each "data reference" in publications intheir area of expertise. For each, they evaluated it against thementions and citations output by the model of each team that foundmentions or citations in the selected publication. For eachreference-team pair, the coder flagged any mentions or citations theydeemed "related to" the current data reference[^6].![image3.png](combined_images/chap05_figure3.png){width="6.5in" height="5.0in"}As onewould expect, while we got coders on the same page, each had subtlydifferent ideas about what was or was not "related to". To remove someof this variability from our final data, we then had a sole experiencedresearcher who understood what we were trying to do review all codingand, when he saw coding that obviously did not fit his understanding,either: revise to fit his understanding of "related to"; or flag as onehe was unsure of and note his thoughts. This experienced researcher alsoserved as a final reviewer of the data references that were collected,marking any that did not actually refer to data as needing to be removedfrom our final analysis. Finally, the protocol designer reviewed allremoved data references, corrections, and ambiguities flagged foradditional review, and made a final set of corrections.Scoring the Results-------------------To create a "related to" confusion matrix for each team, we started witha list of all of the data references that our final reviewers indicatedshould be included in our analysis (165 total). We created found-or-not(1 or 0) vectors with a value for every reference set to 1 for thebaseline, and then set based on our coding for each team. For eachpublication, we also included a false positive item that was always 0for the baseline, and that was set to 1 for a given team if they had anymentions or citations that were not "related to" a data reference fromthat publication.We did not develop a way to capture true negatives since the metrics weused to evaluate did not require it.Lessons learnedThe docker-based model submission process worked well for competition,but subsequent use of the models by Digital Science and Bundesbank hasshown that more precise design of how the models work within theirdocker container and the APIs they provide is necessary if packagedmodels are to be be used to produce reusable APIs. For example, to bereadily able to be used within an existing environment, the model needsto be able to be invoked from a simple unit of code (a python function,for example), rather than needing to spin up an instance of a containereach time you want results.To facilitate re-use, we need much more detailed specification of howthe participants should implement their models. For example:If a submission is implementing multiple tasks, each should be brokeninto its own separate API so it can be used separately (so separateservices for mention detection, field detection, and data detection).We need to better specify how we expect the models to be re-trained, inparticular elements of the model we expect to be easily changed andwhich we expect would require a full retraining to tune. For example, wehoped to be able to easily switch out the data sets of interest that aredetected specifically without needing to retrain on a full corpusreferring to those data sets, but we didn't mention this, and none ofthe models worked this way.We also learned that that while the data for the competition was anexcellent starting point, it has some drawbacks. The base ICPSR data didnot include mention text. It only identified ICPSR data that were usedfor analysis. Hence, for the majority of data sets, the only textavailable for characterizing a data set was the title and a paragraph ofdescription with no examples of how the data would be discussed within apublication. A further drawback was that while data signatures ofinterest in the real world might just be clusters of key terms without aformal title, the competition data did not have that information.We consider the competition design to have been effective. We got a goodnumber of participants, the resulting models were interesting and someof the solutions were novel and surprisingly effective given theirnovelty, and discussions after the competition lead to collaborationsbetween pairs of sponsors and participants and collective work on makinga gold standard corpus that could be used to develop better models inthe future (a great step toward higher quality models). The models alsoended up being re-usable as they are, though in a limited scope, andBundesbank has been able to run them and get output of high enoughquality that it is useful to them.There remains substantial work needed to move this effort forward,however. The next iteration of the competition is tentatively scheduledto begin at the end of 2020, and in this round, we are exploring optionsfor building out a better corpus that combine manual, automated andcrowd-sourced means of annotating data. We are working on a morestandardized and carefully designed model packaging framework, tofacilitate re-use. We are also working on more detailed specificationsof model requirements (ability to retrain on data sets of interestwithout needing a whole new corpus of train-test data, for example).*Jonathan Morgan designed and implemented the data annotation andevaluation strategies and the first draft of our model packagingframework. Andrew Gordon sampled and prepared the corpus for processingand for distribution to participants. Ekaterina Levitskaya helped withthe design of refinement of coding protocols and did a substantialamount of the data annotation. Jonathan and Andrew worked together tocollect and run submissions for the competition and summarize the outputfor the judges.*[^1]: Soboroff, I. M., Ounis, I., Lin, J., & Macdonald, C. (2013).    Overview of the TREC--2012 Microblog Track. NIST Special Publication    500--298: The Twenty-First Text REtrieval Conference Proceedings    (TREC 2012), 2012, 20. Retrieved from    [[https://www.nist.gov/publications/overview-trec--2012-microblog-track]{.underline}](https://www.nist.gov/publications/overview-trec%E2%80%932012-microblog-track)[^2]: See    [[http://www.patentsview.org/community/workshop-2015]{.underline}](http://www.patentsview.org/community/workshop-2015)[^3]: For details about the metadata provided for each type of data, see    [[https://github.com/Coleridge-Initiative/rich-context-competition/wiki/Dataset-Description]{.underline}](https://github.com/Coleridge-Initiative/rich-context-competition/wiki/Dataset-Description)[^4]: For more details, including an FAQ that provides guidance on    specific issues that arose during coding, e.g., how to handle data    sets that span multiple years, see the content analysis protocol    [[https://docs.google.com/document/d/1xuZL\_-z1re6TO3Sv8\_9tdFk7z6ovyqTwDVgc1bYO3Ag/edit?usp=sharing]{.underline}](https://docs.google.com/document/d/1xuZL_-z1re6TO3Sv8_9tdFk7z6ovyqTwDVgc1bYO3Ag/edit?usp=sharing)[^5]: [[https://github.com/Coleridge-Initiative/rich-context-competition]{.underline}](https://github.com/Coleridge-Initiative/rich-context-competition)[^6]: The protocol is described in    [[https://docs.google.com/document/d/1Hi13N6gfiRz9nfwCoUQrey8v\_ozY7fKHMtHV4GgX2ys/edit]{.underline}](https://docs.google.com/document/d/1Hi13N6gfiRz9nfwCoUQrey8v_ozY7fKHMtHV4GgX2ys/edit)