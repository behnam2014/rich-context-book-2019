---# Chapter 6 - Allen Institute for Artificial Intelligence (AI2)---author:- |    Daniel King, Waleed Ammar, Iz Beltagy, Christine Betts      **Suchin Gururangan and Madeleine van Zuylen**            Allen Institute for Artificial Intelligence, Seattle, WA, USA      daniel  @allenai.org  bibliography:- 'acl2015.bib'title: The AI2 Submission at The Rich Context Competition---[Introduction](#sec:intro)============The Allen Institute for Artificial Intelligence (AI2) is a non-profitresearch institute founded by Paul G. Allen with the goal of advancingartificial intelligence research for the common good. One of the majorundertakings at AI2 is to develop an equitable, unbiased softwareplatform (Semantic Scholar)[^1] for finding relevant information in thescientific literature. Semantic Scholar extracts meaningful structuresin a paper (e.g., images, entities, relationships) and links them toother artifacts when possible (e.g., knowledge bases, GitHubrepositories), hence our interest in the rich context competition (RCC).In particular, we participated in the RCC in order to explore methodsfor extracting and linking datasets used in papers. At the time of thiswriting, Semantic Scholar comprehensively covers the computer scienceand biomedical literature, and we plan to expand our coverage in 2019 toother scientific areas, including social sciences.In the following sections, we describe our approach to the three tasksof the RCC competition:1. extracting the datasets used in publications,2. predicting the field of research of publications3. extracting the methods used in publications[Methods](#sec:methods)=======[Dataset Extraction and Linking](#sec:datasets_methods)------------------------------This task focuses on identifying datasets used in a scientific paper.Datasets which are merely mentioned but not used in the research paperare not of interest. This task has two sub-tasks:1.  Citation prediction: extraction and linking to a provided knowledge    base of *known datasets*, and2.  Mention prediction: extraction of both *known and unknown* dataset    mentions.### Provided Data ###The provided knowledge base of known datasets includes approximately 10Kdatasets used in social science research. The high textual similarity betweendifferent datasets in the knowledge base informs our approach for linking datasetmentions to their dataset in the knowledge base. Approximately 10% of the datasets in theknowledge base were linked one or more times in the provided corpus of5K papers. To attempt to generalize mention discovery beyond those present in the knowledge base, we train a Named Entity Recognition model on the noisy annotations provided by the labeled mentions in the knowledge base.![image](combined_images/datasets.png){width="13cm"}We provide a high-level overview of our approach in Figure[\[fig:datasets\]](#fig:datasets){reference-type="ref"reference="fig:datasets"}. First, we use a named entity recognition(NER) model to predict dataset mentions. For each mention, we generate alist of candidate datasets from the knowledge base. We also developed arule based extraction system which searches for dataset mentions seen inthe training set, adding the corresponding dataset IDs in the trainingset annotations as candidates. We then use a binary classifier topredict which of these candidates is a correct dataset extraction.Next, we describe each of the sub-components in more detail.### Mention and Candidate Generation ###We first constructed a set of rule based candidate citations by exactstring matching mentions and dataset names from the provided knowledgebase. We found this to have high recall on the provided development foldand our own development fold that we created. However, after our testsubmission, it became clear that there were many datasets in the actualtest set that did not have mentions in the provided knowledge base.To address this limitation, we developed an NER model to predictadditional dataset mentions. For NER, we use a bi-LSTM model with a CRFdecoding layer, similar to [@Peters2018DEEPCW], and implemented usingthe AllenNLP framework.[^2] In order to train the NER model, weautomatically generate mention labels by string matching mentions in theprovided annotations against the full text of a paper. This results innoisy labeled data, because it was not possible to find all correctmentions this way (e.g., some dataset mentions were not annotated), andthe same string can appear multiple times in the paper, while only someare correct examples of dataset usage.We limit the percentage of negative examples (i.e., sentences with nomentions) used in training to 50%, and use 40 words as the maximumsentence length. We use 50-dimensional Glove word embeddings[@Pennington2014GloveGV], 16-dimensional character embeddings with 64CNN filters of sizes (2, 3, 4). The CNN character encoder outputs128-dimensional vectors. We optimize model parameters using ADAM[@Kingma2014AdamAM] with a learning rate of 0.001.In order to generate linking candidates for the NER mentions, we scoreeach candidate dataset based on TF-IDF weighted token overlap between the mentiontext and the dataset title. For a given mention, many dataset titles canhave a non-zero overlap score, so we take the top 30 scoring candidatesfor each mention as the linking candidates for that mention.### Candidate Linking ###The linking model takes as input a dataset mention, its context, and oneof the candidate datasets in the knowledge base, and outputs a binarylabel. We use a gradient boosted trees classifier using the XGBoostimplementation.[^3] The model takes as input the following features: prior probability ofentity, prior probability of entity given mention, prior probability ofmention given entity, whether a year appears in the mention context andin the dataset title, mention length, mention sentence length, whetherthe mention is an acronym, estimated section title of the mention,overlap between mention context and dataset keywords provided in theknowledge base, and the TF-IDF weighted token overlap. We note that itis possible to predict zero, one or multiple dataset IDs for the samemention, and each dataset candidate is scored independently.[Research Area Prediction](#sec:areas_methods)------------------------### Data ###The second task of the competition is to predict research areas of apaper. The task does not specify the set of research areas of interest,nor is training data provided for the task. After manual inspection of asubset of the papers in the provided test set, the SAGE taxonomy ofresearch, and the Microsoft Academic Graph (MAG) [@Shen2018AWS], wedecided to use a subset of the fields of study in MAG as labels. Inparticular, we included all fields related to social science or papersfrom the provided training corpus. However, since the abstract and fulltext of papers are not provided in MAG, we only use the paper titles fortraining our model. The training data we ended up with includedapproximately 75K paper titles along with their fields of study asspecified in two levels of the MAG hierarchy. We held out about 10% ofthe titles for development data. The coarse level (L0) has 7 fieldswhile the more granular one (L1) has 32. Fields associated with lessthan 100 papers were excluded.### Methods ###For each level, we trained a bi-directional LSTM which reads the papertitle and predicts one of the fields in this level. We additionallyincorporate ELMo embeddings [@Peters2018DEEPCW] to improve performance.In the final submission, we always predict the most likely field fromthe L0 classifier, and only report the most likely field from the L1classifier if it exceeds a certain threshold. It takes approximately 1.5and 3.5 hours for the L0 and L1 classifiers to converge, respectively.[Research Method Extraction](#sec:methods_methods)--------------------------### Data ###The third task in the competition is to extract the scientific methodsused in the research paper. Since no training data was provided, westarted by inspecting a subset of the provided papers to get a betterunderstanding of what kind of methods are used in social science and howthey are referred to within papers.### Methods ###Based on the inspection, we designed regular expressions which capturecommon contextual patterns as well as the list of provided SAGE methods.In order to score candidates, we used a background corpus to estimatethe salience of candidate methods in a paper. Two additional strategieswere attempted but proved unsuccessful: a weakly-supervised model fornamed entity recognition, and using open information extraction (openIE)to further generalize the list of candidate methods.[Results](#sec:results)============[Dataset Extraction and Linking](#sec:datasets_results)------------------------------First, we report the results of our NER model in Table[\[tab:ner\_results\]](#tab:ner_results){reference-type="ref"reference="tab:ner_results"}. Since it is easy for the model to memorizethe dataset mentions seen at training time, we created disjoint train,development, and test sets based on the paper--dataset annotationsprovided for the competition. In particular, we sort datasets by thenumber of papers they appear in, then process one dataset at a time. Foreach dataset, we choose one of the train, development, or test splits atrandom and add the dataset to it, along with all papers which mention that dataset. When there is aconflict, (e.g., a paper *p* has already been added to thetrain split when processing an earlier dataset *d<sub>1</sub>*, but it isalso associated with a later dataset *d<sub>2</sub>*), the later dataset*d<sub>2</sub>* along with all papers associated with it are added to thesame split as *d<sub>1</sub>*. For any further conflicts, we prefer toput papers in the development split over the train split, and the testsplit over the development split.We also experimented with adding ELMo embeddings [@Peters2018DEEPCW],but it significantly slowed down training and decoding which would havedisqualified our submission due to the runtime requirements of thecompetition. As a result, we decided not to include ELMo embeddings inour final model.|            | prec.   | recall  |  F1    || ---------- | ------- | ------- | ------ || dev set    | 53.4    | 50.3    | 51.8   || test set   | 50.7    | 41.8    | 45.8   |NER precision, recall and F1 performance (%) on the development andtest sets.[\[tab:ner\_results\]]{#tab:ner_results label="tab:ner_results"}|                                   | prec.   | recall   |  F1  || --------------------------------- | ------- | -------- | ---- || baseline                          | 28.7    | 58.0     | 38.4 || \+ p(d $\mid$ m), p(m $\mid$ d)   | 39.6    | 42.0     | 40.7 || \+ year matching                  | 35.1    | 57.0     | 43.5 || \+ aggregated mentions, tuning, and other features | 72.5 | 45.0 | 55.5 || \+ dev set examples               | 77.0    | 47.0     | 58.3 || \+ NER mentions                   | 56.3    | 62.0     | 59.0 |End-to-end precision, recall and F1 performance (%) for citationprediction on the development set provided in phase 1 of thecompetition.[\[tab:e2e\_results\]]{#tab:e2e_results label="tab:e2e_results"}|                   | prec.   | recall   |  F1    || ----------------- | ------- | -------- | ------ || phase 1 holdout   | 35.7    | 19.6     | 25.3   || phase 2 holdout   | 39.6    | 18.8     | 25.5   |End-to-end precision, recall, and F1 performance (%) for datasetprediction on the phase 1 and phase 2 holdout sets. Note that thephase 1 holdout results are for citation prediction, while the phase 2 holdout results are for mention prediction.[\[tab:test\_results\]]{#tab:test_results label="tab:test_results"}We report the end-to-end performance of our approach (on the developmentset provided by the organizers in the first phase) in Table[\[tab:e2e\_results\]](#tab:e2e_results){reference-type="ref"reference="tab:e2e_results"}. This is the performance after using thelinking classifier to predict which candidate mention--dataset pairs arecorrect extractions. We note that the development set provided in phase1 ended up having significantly more overlap with the training data thanthe actual test set did. As a result, the numbers reported in Table[\[tab:e2e\_results\]](#tab:e2e_results){reference-type="ref"reference="tab:e2e_results"} are not indicative of test set performance.End to end performance from our phase 2 submission can be seen in Table[\[tab:test\_results\]](#tab:test_results){reference-type="ref"reference="tab:test_results"}. This performance is reflective of ourfocus on the linking component of this task. Aside from the competitiondevelopment set, we also used a random portion of the training set as anadditional development set. The initial model only uses a datasetfrequency feature, which gives a baseline performance of 38.4 F1. Addingp(d $\mid$ m) and p(m $\mid$ d), which are the probability of entitygiven mention and probability of mention given entity improves theperformance ($\Delta = 2.3$ F1). Year matching helps disambiguatebetween different datasets in the same series, which was found to be amajor source of errors in earlier models ($\Delta = 2.8$ F1).Aggregating mentions for a given dataset, adding mention and sentencelength features, adding an is acronym feature, and furtherhyper-parameter tuning improve the results ($\Delta = 12.5$ F1). Addingexamples in the development set while training the model results infurther improvements ($\Delta = 2.8$ F1). Finally, adding the NER-basedmentions significantly improves recall at the cost of lower precision,with a positive net effect on F1 score ($\Delta = 0.7$ F1).Two clear limitations of our model are its difficulty in generalizing to unseen datasets, and its inability to effectively distinguish between datasets that are used in a publication and datasets that are merely reference. These limitations are the main causes of the low recall (due to difficulty generalizing to unseen datasets) and low precision (due to difficulty distinguishing between used datasets and referenced datasets).[Research Area Prediction](#sec:areas_results)------------------------To select a model, we performed a 100 trial random search across modelhyper-parameters, evaluated on a held out development set of papers fromthe Microsoft Academic Graph. Our final model contained 512 hiddendimensions, 2 layers and 0.5 dropout prior to classification. The topperforming classifier achieved 84.4% accuracy on our development set onL0 fields, and 65.2% accuracy on our development set on L1 fields. The main limitation of using MAG for this problem is that our model cannot find new fields of research, and is limited to those provided by MAG. Additionally, our method performs classification based only on the titles of papers, while there are other pieces of information about the paper that would be useful for classifying the field of research.[Research Method Extraction](#sec:methods_results)--------------------------We evaluated performance by manually evaluating the output of ourextractor for a subset of 50 papers from the provided test set tocompute precision. Since evaluating recall requires a carefulannotation, we resorted to using yield as an alternative metric. Ourfinal submission for method extraction has 95% precision and yield of1.5 methods per paper on the manually inspected subset of papers. Similarly to research area prediction, the main limiation here is the difficulty our model has finding new methods, as it is limited to the SAGE ontology and a few hand-crafted patterns.[Future Work and Lessons Learned](#sec:future_work)============We now provide some possible directions of improvement for eachcomponent of our submission. For dataset extraction, the most promisingavenue of improvement is to improve the NER model, and the mostpromising avenue to improve the NER model is to collect less noisy data.We effectively have distantly supervised training data for the NERmodel, and the first thing to try would be directly annotating paperswith dataset mentions to provide a clearer signal for the NER model. Forresearch area prediction, it would help to include signals beyond justthe paper title for predicting the field of study. The difficulty hereis finding labeled training data that includes richer signals likeabstract text and paper keywords. For method prediction, exploringthe use of open information extraction is a potential avenueof future research. Additionally, it would be helpful to clarify whatexactly is meant by a method, as it is currently unclear what asuccessful method extraction looks like.The main lesson learned is that, when presented with noisy, distantly supervised, real-world data, to produce a production-quality system, it becomes very important to (1) have a high-confidence evaluation dataset, and (2) look for other data sources that are similar enough to the task at hand to be useful. Taking steps towards both of these objectives are promising avenues of future work.[Conclusion](#sec:conclusion)==========This report summarizes the AI2 submission at the RCC competition. Weidentify dataset mentions by combining the predictions of an NER modeland a rule-based system, use TF-IDF to identify candidates for a givenmention, and use a gradient boosted trees classifier to predict a binarylabel for each candidate mention--dataset pair. To identify researchfields of a paper, we train two multi-class classifiers, one for each ofthe top two levels in the MAG hierarchy for fields of study. Finally, to extract research methods, weuse a rule-based system utilizing a dictionary and common patterns,followed by a scoring function which takes into account the prominenceof a candidate in foreground and background corpora.[Acknowledgments](#sec:acknowledgements)===============We would like to thank the competition organizers for their tirelessefforts in preparing the data, answering all our questions, doing theevaluations, and providing feedback. We also would like to thank Zhihong(Iris) Shen for helping us use the MAG data.[Appendix](#sec:appendix)========The code for the submission can be found [here](https://github.com/allenai/coleridge-rich-context-ai2). There is a README with additional documentation at this github repo.[^1]: [www.semanticscholar.org](www.semanticscholar.org)[^2]: <https://github.com/allenai/allennlp/blob/master/allennlp/models/crf_tagger.py>[^3]: <https://xgboost.readthedocs.io/en/latest/>