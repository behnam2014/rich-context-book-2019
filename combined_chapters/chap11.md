---# Chapter 11 - Syracuse---abstract: |    Datasets are critical for scientific research, playing a role in    replication, reproducibility, and efficiency. Researchers have recently    shown that datasets are becoming more important for science to function    properly, even serving as artifacts of study themselves. However, citing    datasets is not a common or standard practice in spite of recent efforts    by data repositories and funding agencies. This greatly affects our    ability to track their usage and importance. A potential solution to    this problem is to automatically extract dataset mentions from    scientific articles. In this work, we propose to achieve such extraction    by using a neural network based on a BiLSTM-CRF architecture. Our method    achieves $F_{1}=0.885$ in social science articles released as part of    the Rich Context Dataset. We discuss future improvements to the model    and applications beyond social sciences.author:- 'Tong Zeng$^{1,2}$ and Daniel Acuna$^{1}$[^1]'bibliography:- 'rcc-06.bib'title: |    Dataset mention extraction in scientific articles using a BiLSTM-CRF    model---Introduction============Science is fundamentally an incremental discipline that depends onprevious scientist’s work. Datasets form an integral part of thisprocess and therefore should be shared and cited as any other scientificoutput. This ideal is far from reality: the credit that datasetscurrently receive does not correspond to their actual usage. One of theissues is that there is no standard for citing datasets, and even ifthey are cited, they are not properly tracked by major scientificindices. Interestingly, while datasets are still used and mentioned inarticles, we lack methods to extract such mentions and properlyreconstruct dataset citations. The Rich Context Competition challengeaims at closing this gap by inviting scientists to produce automateddataset mention and linkage detection algorithms. In this article, wedetail our proposal to solve the dataset mention step. Our approachattempts to provide a first approximation to better give credit and keeptrack of datasets and their usage.The problem of dataset extraction has been explored before.@ghavimiIdentifyingImprovingDataset2016 and@ghavimiSemiautomaticApproachDetecting2017 use a relatively simpletf-idf representation with cosine similarity for matching datasetidentification in social science articles. Their method consists of fourmajor steps: preparing a curated dictionary of typical mention phrases,detecting dataset references, and ranking matching datasets based oncosine similarity of tf-idf representations. This approach achieved arelatively high performance, with $F_{1}=0.84$ for mention detection and$F_{1}=0.83$, for matching. @singhalDataExtractMining2013 proposed amethod using normalized Google distance to screen whether a term is in adataset. However, this method relies on external services and is notcomputational efficient. They achieve a good $F_{1}=0.85$ using Googlesearch and $F_{1}=0.75$ using Bing. A somewhat similar project wasproposed by @luDatasetSearchEngine2012. They built a dataset searchengine by solving the two challenges: identification of the dataset andassociation to a URL. They build a dataset of 1000 documents with theirURLs, containing 8922 words or abbreviations representing datasets. Theyalso build a web-based interface. This shows the importance of datasetmention extraction and how several groups have tried to tackle theproblem.In this article, we describe a method for extracting dataset mentionsbased on a deep recurrent neural network. In particular, we used aBidirectional Long short-term Memory (BiLSTM) sequence to sequence modelpaired with a Conditional Random Field (CRF) inference mechanism. Wetested our model on a novel dataset produced for the Rich ContextCompetition challenge. We achieve a relatively good performance of$F_{1}=0.885$. We discuss the limitations of our model.The dataset===========The Rich Context Dataset challenge was proposed by the New YorkUniversity’s Coleridge Initiative [@richtextcompetition]. The challengecomprised several phases, and participants moved through the phasesdepending on their performance. We only analyze data of the first phase.This phase contained a list of datasets and a labeled corpus of around5K publications. Each publication was labeled indicating whether adataset was mentioned within it and which part of the text mentioned it.The challenge used the accuracy for measuring the performance of thecompetitors and also the quality of the code, documentation, andefficiency.We adopt the CoNLL 2003 format [@tjong2003introduction] to annotatewhether a token is a part of dataset mention. Concretely, B-DS denotes atoken that is the first token of a dataset mention, I-DS denotes a tokenthat is inside of dataset mention, and O denotes a token that is not apart of dataset mention. We then put each token and its correspondinglabels in one line and use a empty line as a separator betweensentences. Sentences were randomly split by 70%, 15%, 15% for trainingset, validation set and testing set, respectively.The Proposed Method===================Overall view of the architecture--------------------------------In this section, we propose a model for detecting mentions based on aBiLSTM-CRF architecture. At a high level, the model uses asequence-to-sequence recurrent neural network that produces theprobability of whether a token belongs to a dataset mention. The CRFlayer takes those probabilities and estimates the most likely sequencebased on constrains between label transitions (e.g.,mention–to–no-mention–to-mention has low probability). While this is astandard architecture for modeling sequence labeling, the application toour particular dataset and problem is new.We now describe in more detail the choices of word representation,hyper-parameters, and training parameters. A schematic view of the modelis in Fig \[fig:NetworkArchitecture\] and the components are as follows:1.  Character embedding layer: treat a token as a sequence of characters    and encode the characters by using a bidirectional LSTM to get a    vector representation.2.  Word embedding layer: mapping each token into fixed sized vector    representation by using a pre-trained word vector.3.  One BiLSTM layer: make use of Bidirectional LSTM network to capture    the high level representation of the whole token sequence input.4.  Dense layer: project the output of the previous layer to a low    dimensional vector representation of the the distribution of labels.5.  CRF layer: find the most likely sequence of labels.![\[fig:NetworkArchitecture\]Network Architecture of BiLSTM-CRFnetwork](combined_images/bilistm_crf_network_structure_pic){width="80.00000%"}Character Embedding-------------------Similar to the bag of words assumption, we can consider a token iscomposed by a bag of characters. In this layer, we convert each token toa sequence of characters, then feed the sequence into a bidirectionalLSTM network to get a fixed length representation of the token. Afterlearning the bidirectional LSTM network, we can solve theout-of-vocabulary problem for pre-trained word embeddings.Word Embedding--------------The embedding is the first layer of our network and it is responsiblefor mapping the word from string into vectors of numbers as the inputfor other layers on top. For a given sentence $S$, we first convert itinto a sequence consisting of $n$ tokens,$S=\{c_{1},c_{2},\cdots,c_{n},\}$ . For each token $c_{i}$we lookup theembedding vector $x_{i}$ from a word embedding matrix$M^{tkn}\in\mathbb{R}^{d|V|}$, where the $d$ is the dimension of theembedding vector and the $V$ is the Vocabulary of the tokens. In thispaper, the matrix $M^{tkn}$ is initialized by pre-trained GloVe vectors[@pennington2014glove], but will be updated by learning from our corpus.LSTM----Recurrent neural network (RNN) is a powerful tool to capture featuresfrom sequential data, such as temporal series, and text. RNN couldcapture long-distance dependency in theory but it suffers from thegradient exploding/vanishing problems [@pascanu2013difficulty]. The Longshort-term memory (LSTM) architecture was proposed by@hochreiter1997long and it is a variant of RNN which copes with thegradient problem. LSTM introduces several gates to control theproportion of information to forget from previous time steps and to passto the next time step. Formally, LSTM could be described by thefollowing equations:$$i_{t}=\sigma(W_{i}x_{t}+W_{i}h_{t-1}+b_{i})$$$$f_{t}=\sigma(W_{f}x_{t}+W_{f}h_{t-1}+b_{f})$$$$g_{t}=tanh(W_{g}x_{t}+W_{g}h_{t-1}+b_{g})$$$$o_{t}=\sigma(W_{o}x_{t}+W_{o}h_{t-1}+b_{o})$$$$c_{t}=f_{t}\bigotimes c_{t-1}+i_{t}\bigotimes g_{t}$$$$h_{t}=o_{t}\bigotimes tanh(c_{t})$$where the $\sigma$ is the sigmoid function, $\bigotimes$ denotes the dotproduct, $b$ is the bias, $W$ is the parameters, $x_{t}$ is the input attime $t$, $c_{t}$ is the LSTM cell state at time $t$ and $h_{t}$ ishidden state at time $t$. The $i_{t}$, $f_{t}$, $o_{t}$ and $g_{t}$ arenamed as input, forget, output and cell gates respectively, they controlthe information to keep in its state and pass to next step.LSTM gets information from the previous steps, which is left context inour task. However, it is important to consider the information in theright context. A solution of this information need is bidirectional LSTM[@graves2013speech]. The idea of Bi-LSTM is to use LSTM layers and feedthe forward and backward flows separately, and then concatenate thehidden states of the two LSTM to modeling both the left and rightcontexts$$h_{t}=[\overrightarrow{h_{t}}\varoplus\overleftarrow{h_{t}}]$$Finally, the outcomes of the states are taken by a Conditional RandomField (CRF) layer that takes into account the transition nature of thebeginning, intermediate, and ends of mentions. For a reference of CRF,refer to [@lafferty2001conditional]Results=======In this work, we wanted to propose a model for the Rich ContextCompetition challenge. We propose a relatively standard architecturebased on a BiLSTM-CRF recurrent neural network. We now describe theresults of this network on the dataset provided by the competition.For all of our results, we use $F_{1}$ as the measure of choice. Thismeasure is the harmonic average of the precision and recall and it isthe standard measure used in sequence labeling tasks. This metric variesfrom 0 to 1, and the unit is the highest possible value. Our methodachieved a relatively high $F_{1}$ of 0.885 for detecting mentions, inline with previous studies.   Models   GloVe size   Dropout rate   Precision   Recall    $F_{1}$  -------- ------------ -------------- ----------- -------- -----------     m1         50           0.0          0.884     0.873      0.878     m2         50           0.5          0.877     0.888      0.882     m3        100           0.0          0.882     0.871      0.876     m4        100           0.5          0.885     0.885    **0.885**     m5        200           0.0          0.882     0.884      0.883     m6        200           0.5          0.885     0.880      0.882     m7        300           0.0          0.868     0.886      0.877     m8        300           0.5          0.876     0.878      0.877  : \[tab:Performance-of-proposed\]Performance of proposed networkWe train models using the training data, monitor the performance usingthe validation data (we stop training if the performance doesn’t improvefor the last 10 epochs). We are using the Adam optimizer with learningrate of 0.001 and batch size equal to 64. The hidden size of LSTM forcharacter and word embedding is 80 and 300, respectively. For theregularization methods to avoid over-fitting, we use L2 regularizationwith alpha set to 0.01, we also use dropout rate equal to 0.5. Wetrained 8 models with a combination of different GloVe vector size (50,100, 300 and 300) and dropout rate (0.0, 0.5). The performances arereported on the test dataset in Table \[tab:Performance-of-proposed\].The best model is trained by word vector size 100 and dropout rate 0.5with $F_{1}$ score 0.885.We also found some limitations to the dataset. Firstly, we found thatmentions are nested (e.g. HRS, RAND HRS, RAND HRS DATA are linked to thesame dataset). The second issue most of the mentions have ambiguousrelationships to datasets. In particular, only 17,267 (16.99%) mentionsare linked to one dataset, 15,292 (15.04%) mentions are listed to twodatasets, and 12,624 (12.42%) are linked to three datasets. If thesedifficulties are not overcome, then the predictions from the linkageprocess will be noisy and therefore impossible to tell apart.Conclusion==========In this work, we report a high accuracy model for the problem ofdetecting dataset mentions. Because our method is based on a standardBiLSTM-CRF architecture, we expect that updating our model with recentdevelopments in neural networks would only benefit our results. We alsoprovide some evidence of how difficult we believe the linkage step ofthe challenge could be if the dataset noise are not lowered.One of the shortcomings of our approach is that the architecture islacking some modern features of RNN networks. In particular, recent workhas shown that attention mechanisms are important especially when thetask requires spatially distant information, such as this one. Thesebenefits could also translate to better linkage. We are exploring newarchitectures using self-attention and multiple-head attention. We hopeto explore these approaches in the near future.Our proposal, however, is surprisingly effective. Because we have barelymodified a general RNN architecture, we expect that our results willgeneralize relatively well either to the second phase of the challengeor even to other disciplines. We would emphasize, however, that thequality of the dataset has a great deal of room for improvement. Givenhow important this task is for the whole of science, we should try tostrive to improve the quality of these datasets so that techniques likethis one can be more broadly applied. The importance of dataset mentionand linkage therefore could be fully appreciated by the community.Acknowledgements {#acknowledgements .unnumbered}================Tong Zeng was funded by the China Scholarship Council \#201706190067.Daniel E. Acuna was funded by the National Science Foundation awards\#1646763 and \#1800956.[^1]: Corresponding author: deacuna@syr.edu