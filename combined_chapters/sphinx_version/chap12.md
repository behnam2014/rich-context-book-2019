---# Chapter 12 - Standard Test CorporaRich Context Book Chapter - Standardized Metadata, Full Text andTraining/Evaluation for Extraction Models\[**Standardized Metadata & Full Text \[Sebastian\]**]{.s1}\Key challenges when working on an NLP task like dataset mentionextraction that requires access to scholarly literature include theproliferation of metadata sources and sourcing of full text content. Forexample, each metadata source has their own approach for disambiguation(e.g. recognizing that A. Smith and Anna Smith are the same author) orde-duplication of content (clustering pre-prints and final versions intoa single record). As a result competition organizers and NLP researcherscurrently use ad-hoc processes to identify metadata and full textsources for their specific tasks which results in inconsistencies and alack of versioning of input data across competitions and projects.\One way these challenges can be addressed is by using a trustworthymetadata source like [[Semantic Scholar’s opencorpus]{.s2}](http://api.semanticscholar.org/corpus/) developed by theAllen Institute for Artificial Intelligence (AI2) or [[Microsoft’sAcademicGraph]{.s2}](https://docs.microsoft.com/en-us/academic-services/graph/reference-data-schema)that make it easy to access standardized metadata from an openlyaccessible source. In addition, both Semantic Scholar and the MicrosoftAcademic Graph provide topics associated with papers which makes it easyto narrow down papers by domain. If full text is needed we recommendtying the metadata to a source of open access full text content like[[Unpaywall]{.s2}](https://unpaywall.org/data-format) to ensure that thefull text can be freely redistributed and leveraged for modeldevelopment.\To gather the data we recommend collecting a sufficiently large set offull text papers (3,000-5,000 minimum) with their associated metadataand providing participants with a standardized format of the full text.More data might be required if data is split across many scientificdomains. For example for a task like dataset extraction, referenceformatting is often inconsistent across domains and dataset mentions canpotentially be found in different sections (e.g. background, methods,discussion, conclusion or the reference list) throughout the text. Oncea decision has been made on the full text to include, the PDF contentcan be easily converted into text in a standardized format using a PDFto text parser like [[AI2’sScienceParse]{.s2}](https://github.com/allenai/spv2) (which handles keytasks like metadata, section heading and referencesextraction).\Once the metadata and full text dataset has been created it can beeasily versioned and used again in future competitions. For example, ifupdated metadata is needed it’s easy to go back to the original metadatasource (for example by using Semantic Scholar’s[[API]{.s2}](http://api.semanticscholar.org/)) to get the latestmetadata.\[**Annotation Protocols to Produce Training & Evaluation Data\[Alex\]**]{.s1}A common approach to machine learning known as **supervised learning**uses labelled, or annotated, data to train a model what to look for. Iflabelled data is not readily available, human annotators are frequentlyused to label, or code, a corpus of representative document samples asinput into such a model. Different labelling tasks may require differentlevels of subject domain knowledge or expertise. For example, coding adocument for different parts of speech (POS) will require a differentlevel of knowledge than coding a document for mentions of upregulationof genes. The simpler the labelling task, the easier it will be for thecoders to complete the task, and the more likely the annotations will beconsistent across multiple coders.Forexample, a task to identify a *mention of a dataset* in a document mightbe far easier than the task of identifying only the*mentions of**datasets that were used in the analysis phase ofresearch*.\In order to scale the work of labelling, it is usually desirable todistribute the work amongst many people. Generic crowdsourcing platformssuch as Amazon’s Mechanical Turk can be used in some labellingexercises, as can more tailored services from companies such as TagWorksand Figure-Eight. Whether the labelling is done by one person orthousands, the consistency and quality of the annotations needs to beconsidered. We would like to build up a sufficiently large collection ofthese annotations and we want to ensure that they are of a high quality.How much data needs to be annotated depends on the task, but in general,the more labelled data that can be generated the more robust the modelwill be.\As mentioned above, we recommend 3000-5000 papers, but this begs thequestion of how diverse the subject domains are within this corpus.If the papers are all within from the financesector, then a resulting model might do well in identifying datasets infinance, but less well in the biomedical domain since the model was nottrained on biomedical papers. Conversely, if our 3000-5000 papers areevenly distributed across all domains, our model might be moregenerically applicable, but might do less well over all since it did notcontain enough individual domain-specific examples.  \As a result, we recommend labelling 3000-5000 papers within a domain,but we plan to do so in a consistent manner across domains so that theannotations can be aggregated together. In this manner, as papers in newdomains are annotated, our models can be re-trained to expand into newdomains. In order to achieve this, we intend to publish an openannotation protocol and output format that can be used by the communityto create additional labelled datasets.Another factor in deciding the quantity is the fact that the annotationswill be used for two discrete purposes. The first is to *train* amachine learning model. This data will inform the model what datasetmentions look like, from which it will extract a set of features thatthe model will use and attempt to replicate. The second use of theannotations is to *evaluate* the model.Howwell a model performs against some content that it has never seenbefore. In order to achieve this, labelled data are typically splitrandomly into training and evaluation subsets.One way to evaluate how well your model performs is to measure the**recall** and **precision** of the model’s output, and in order to dothis we can compare the output to the labelled evaluation subset. Inother words, how well does our model perform against the humanannotations that it was not trained on and has never seen. Recall is thepercentage of right answers the model returned. For example, if theevaluation dataset contained 1000 mentions of a dataset, and the trainedmodel returned 800 of them, then the recall value would be .80.But what if the model returned everything as adataset, then it would get all 1000, plus a whole bunch of wronganswers. Obviously, the precision of the model is important too.Precision is the percentage of answers returned that were right. So,continuing the example above, if the model returned 888 answers, and 800of those were right, then the precision of the model would be \~.90.But again, if the model returned only one rightanswer and no wrong ones, the precision would be perfect. So, it isimportant to measure both precision and recall.In summary, the model in this example, got 80%of the right answers, and 90% of the answers it returned were right. Thetwo measures of recall and precision can be combined into an F1 score of  ~.847.  \If we then make modifications to our model, we can re-run it against theevaluation dataset and see how our F1 score changes. If the score goesup, then our new model performed better against this evaluation data. Ifwe want to compare several different models to see which one performedbest, we can calculate an F1 score for each of them. The one with thehighest F1 score has performed the best. Consequently, the quality ofthe annotations are critical for two reasons: first, the accuracy of a*model* will only be as good as the data upon which it was trained. Andsecondly, the accuracy of the *evaluation* (in this case the F1 score)can be affected by the quality of the data it is evaluated against. \\