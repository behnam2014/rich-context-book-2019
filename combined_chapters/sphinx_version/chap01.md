# Chapter 1 - IntroductionRich Context Introductory ChapterIan Mulvany, Paco Nathan, Sophie Rand, Julia LaneThe social sciences are at a crossroads. The enormous growth of thescientific enterprise, coupled with rapid technological progress, hascreated opportunities to conduct research at a scale that would havebeen almost unimaginable a generation or two ago. The rise of cheapcomputing, connected mobile devices, and social networks with globalreach allows researchers to rapidly acquire massive, rich datasets; toroutinely fit statistical models that would once have seemed intractablycomplex; and to probe the way that people think, feel, behave, andinteract with one another in ever more naturalistic, fine-grained ways.Yet much of the core infrastructure is manual and ad-hoc in nature,threatening the legitimacy and utility of social science research.We can and must do better. The great challenges of our time are human innature - terrorism, climate change, the use of natural resources, andthe nature of work - and require robust social science to understand thesources and consequences. Yet the lack of reproducibility andreplicability evident in many fields(*1*--*5*) is even more acute in thestudy of human behavior both because of the difficulty of sharingconfidential data and because of the lack of scientific infrastructure.There is an enormous interest, while only a fraction of datasets areidentified in scientific research, those publications that do cite dataare cited up to 25% more than those that do not(*6*)The central argument we advance in this monograph is that advances intechnology---and particularly, in automation---can now change the way inwhich social science is done. Social scientists have eagerly adopted newtechnologies in virtually every area of social science research---fromliterature searches to data storage to statistical analysis todissemination of results.A major challenge is search and discovery. The vast majority of socialscience data and outputs cannot be easily discovered by otherresearchers even when nominally deposited in the public domain. A newgeneration of automated search tools could help researchers discover howdata are being used, in what research fields, with what methods, withwhat code and with what findings. And automation can be used to rewardresearchers who validate the results and contribute additionalinformation about use, fields, methods, code, and findings.(*7*)Vannevar Bush foreshadowed the issue more than 60 years ago:> "There is a growing mountain of research. But there is increased> evidence that we are being bogged down today as specialization> extends. The investigator is staggered by the findings and conclusions> of thousands of other workers---conclusions which he cannot find time> to grasp, much less to remember, as they appear. ... Mendel's concept> of the laws of genetics was lost to the world for a generation because> his publication did not reach the few who were capable of grasping and> extending it; and this sort of catastrophe is undoubtedly being> repeated all about us, as truly significant attainments become lost in> the mass of the inconsequential"(*8*).This challenge is particularly true for empirical research. Faced with anever-ending stream of new findings and datasets generated usingdifferent code and analytical techniques, researchers cannot readilydetermine who has worked in an area before, what methods were used, whatwas produced, and where those products can be found. Resolving suchuncertainties consumes an enormous amount of time and energy for manysocial scientists. Automated tools and services could greatly facilitatethe process---often by passively capitalizing on the accumulated laborof one's extended research community.In sum, the use of data depends critically on knowing how it has beenproduced and used before: the required elements what do the data***measure***, what ***research*** has been done by what***researchers,*** with what ***code***, and with what ***results***.Acquiring that knowledge has historically been manual and inadequate.The challenge is particularly acute in the case of confidential data onhuman subjects, since it is impossible to provide fully open access tothe source files.Although a field is developing to generate automated approaches, thereare five major scientific challenges: (i) document corpus development,(ii) ontology development for dataset entity classification, (iii)natural language processing and machine learning models for datasetentity extraction, (iv) graph models for improving search and discovery,and (v) the use of the results to engage the community to both validatethe model results, retrain the model and to contribute code andknowledge.Section 1 provides an overview of the motivation and approach. Section 2describes new approaches to develop corpora and ontologies. Section 3describes the competition results in terms of model development. Section4 provides a forward looking agenda.Section 1: Motivation and approachIn Chapter 2, " Where's Waldo: Conceptual issues when characterizingdata in empirical research," researchers from the Research Data andService Center at the Deutsche Bundesbank show us why better metadatafor social science data enables discovery of datasets and research, inways that surpass what traditional metadata from data producers cansupport. They present a new modus operandi in the service delivery modelof research data facilities, based on the premise that datasets have ameasurable value that can be deduced from the relationships betweendatasets and publications, and the people who author, do research on,and consume them - that is, Rich Context around datasets.They argue that a major advantage of rich context is that it closes theloop on metadata is closed: a loop initiated by the metadata from thedata producer side, is closed by metadata from the data usage side. Theauthors elucidate why such rich data from the *usage* perspective isneeded to deliver codified knowledge to the research community to guideliterature review and new research; without understanding the linkagebetween datasets and outcomes, we are disabled in shaping new, impactfulresearch.  The authors identify two primary reasons for this need: first, thatmetadata on the datasets from the data users perspective helps the datacreator to improve upon the quality of the data itself, improvingdataset owners' service delivery (e.g. bundesbank as a service provider,the service being data provision, consulting on dataset usage, creationof new data products, etc); and second, that metadata on the usage ofdatasets in publications helps us measure impact of datasets in theirability to drive policy-making. With this closed loop, the scope ofresearchers' discovery is broadened to include not only literature anddatasets, but the interplay between those two - that is, how datasetshave been used by whom and how.   The authors discuss a tangible outcomeof measuring dataset value - a dataset recommendation system, enablingexpedient sharing of available datasets through the research community.Chapter 3 outlines the operational approach that was taken to developthe [Rich ContextCompetition](https://coleridgeinitiative.org/richcontextcompetition).The goal of the competition, the results of which are described inSection 2, was to implement AI to automatically extract metadata fromresearch - identifying datasets in publications, authors and experts,and methodologies used. As such, the competition was designed to engagepractitioners in AI and NLP to develop models based on a corpusdeveloped at the Interuniversity Consortium of Political and SocialResearch. The competition attracted 20 teams from around the world andresulted in four finalists presenting their results at NYU on February15, 2019 (see the [agenda and videohere](https://coleridgeinitiative.org/richcontextcompetition/workshopagenda)).The results of the competition provided metadata to describe links amongdatasets used in social science research. In other words, the outcome ofthe competition generated the basis for a moderately-sized knowledgegraph. the [winningteam](https://ocean.sagepub.com/blog/an-interview-with-the-allen-institute-for-artificial-intelligence)in the Rich Context Competition was from [AllenAI](https://allenai.org/) which is a leader in the field of usingembedded models for natural language. Typical open source frameworkswhich are popular for deep learning research include[PyTorch](https://pytorch.org/) (from Facebook) and the more recent[Ray](https://ray.readthedocs.io/en/latest/distributed_training.html)(from UC Berkeley RISElab).Section 2:===========A major challenge is developing a training corpus that sufficientlyrepresents the population of all documents, and tagging the datasets inthe corpus. It is essential to do this well if high quality models areto be developed. There is a literature outlining the issues withdeveloping a \"gold standard corpus\" (GSC) of language around databeing mentioned and used in analysis in academic publications, sincecreating one is time-consuming and expensive (*9*) In Chapter 4"Standardized Metadata, Full Text and Training/Evaluation for ExtractionModels", Sebastian tk and Alex Wade describe the need for, andstrategies for collecting, large sets of annotated full-text sources foruse as training data for supervised learning models developed in theRich Context Competition. Dataset Extraction, the NLP task at the coreof the Rich Context Competition, relies on a high-quality set of fulltext sources with metadata annotations. Developing such a corpus must bedone strategically, as full-text articles and their metadata areorganized inconsistently across their sources. The corpora gathered foruse as training data for the Competition required ad-hoc manual labor tocompile. Here, authors describe the legal, technological and humanconsiderations in creating a corpus. They dictate the scale of full-textdata needed, and the impact that an interdisciplinary (e.g spanningmultiple domains) corpus has on that scale. They suggest development ofa corpus with open-access text resources, use of human-annotators forlabeling of full-text, and attention to the mix of domains that may bein a corpus when developing models. There is a separate challenge of developing a common understanding ofwhat a dataset is. Developing standard ontologies is a fundamentalscientific problem, and one that is often in the domain of libraries andinformation scientists. Although some measure of linguistic ambiguity islikely to be unavoidable in the social sciences given the complexsubject matter, even modest ontologies that minimally control thevocabulary researchers use would have important benefits. In Chapter 5,"Metadata for Administrative and Social Science Data", Robert B. Allendescribes a framework for the application of metadata to datasets,details existing metadata schema, and gives an overview of thetechnology, infrastructure and human elements that need to be consideredwhen designing a rich metadata schema for describing social sciencedata. Allen describes three types of metadata - structural, administrative anddescriptive; and emphasizes the growth needed in descriptive metadata,which are characterized by semantic descriptions. Allen describesexisting metadata schemas which accommodate domain-specific metadataschema, like the W3C DCAT, and the unique semantic challenges faced bysocial science as opposed to natural sciences - in particular thatconcepts - e.g. "family", "crime" -  are less well-defined, anddefinitions change across sub-domains. He considers data repositoriesand describes the essential role of metadata in making such repositoriessearchable and therefore useful. He touches on several prominent datarepositories in the social and natural sciences and describes theirmethods of gathering metadata and how the metadata supports servicesoffered, like search, computing environments, preservation of data forarchives, and logging of the history of a dataset and its provenance.Allen describes other challengings in creating and maintaining metadata,prompted by things like changes in technology that yield data streams,and changes in metadata standards. He discusses some of the technologyunderlying data repositories; in particular data cubes for data storagethat facilitate data exploration and retrieval; containerization andcloud computing enabling sharing and reproducibility; and collectionmanagement systems which can provide metrics on usage, like number ofdownloads, maintenance of datasets, etc. Section 3:===========Chapter 6, by the Allen AI team, describes their overarching approach.The team used a named entity recognition model to predict datasetmentions. For each mention, they generated a list of candidate datasetsfrom the knowledge base. They also developed a rule based extractionsystem which searches for dataset mentions seen in the training set,adding the corresponding dataset IDs in the training set annotations ascandidates. They then use a binary classiﬁer topredict which of these candidates is a correct dataset extraction. Whilethis approach was eventually the winning approach given the design ofthe corpus and the scoring mechanism, it suffers from being too fragilefor general application, since it is necessarily corpus dependent. Thatteam did not devote substantial time to identifying fields and methods.Chapter 7, by the KAIST team, describes a very different approach. Theygenerated their own questions about dataset names and use a machinelearning technique to train the model for solving question answeringtask. In other words, questions suitable for finding dataset names suchas "What is the dataset used in this paper?," are generated and thequestion answering model is trained to find the answers to thosequestions from the papers. Furthermore, the resulting answers from themodel are filtered by types of each word. For example, if an answercontains words with organization or agency types, then this answer islikely to include the actual dataset names. They also were quiteinnovative with identifying research fields, by using Wikipedia as thesource, and methods by using machine learning techiquesChapter 8, by the GESIS team, also used a Named Entity Recognitionprocedure. However, their design was module-based approach and theydeveloped tools that can be used separately but also as parts of a dataprocessing pipeline. For identifying research methods and fields, theyexploited the Social Science Open Access Repository maintained at GESIS-- Leibniz Institute for the Social Sciences. They also used the ACLAnthology Reference Corpus which is a corpus of scholarly publicationsabout computational linguisticsChapter 9, by the DICE team at Paderborn University, also used a NamedEntity Recognition approach. They trained an Entity Extraction modelbased on Conditional Random Fields and combined it with the results froma Simple Dataset Mention Search to detect datasets in an article. Forthe identification of Fields and Methods, they essentially used searchstring to find embedded wordsChapter 10, by Singapore Management University, was an incompletesubmission, with a very interesting approach. They used datasetdetection followed by implicit entity linking approach to tackle datasetextraction task. They adopt weakly supervised classification forresearch methods and fields identification tasks utilizing SAGEKnowledge as an external source and as a proxy for weak labels.Section 4: Looking forward==========================In Chapter 11, researchers from Digital Science describe the role userengagement plays in creating rich context around datasets, which aretake on properties of 'first class research objects' (like journalarticles) in that they are published as distinct research outputs intheir own right.  Authors lay out a set of challenges in the sharing ofdatasets and dissemination of dataset metadata, and articulate goals increating infrastructure to answer these challenges. As technology has yielded ever larger streams of datasets available forsocial science research, two critical, interrelated elements ofinfrastructure have not kept apace: information infrastructure, andcultural infrastructure.  Information infrastructure refers to contentof interest to the rich context competition models - journal articles,datasets, and their metadata (including details on the data stewards,usage of the datasets in research, and code used to prepare and analyzedatasets). Cultural infrastructure refers to the incentives and valuepropositions in place to encourage individual data stewards, data usersand experts to share datasets and contribute metadata on datasets.Cultural infrastructure around datasets do not fit into the existentculture of research publications. In venturing to build out information infrastructure around datasets, wemust consider how concepts like versioning, reproducibility, and peerreview carry over to datasets. Further, how do metadata carry over, whenthere is so much variability in what we mean when we say dataset?Incentives around data sharing, dataset curation, and metadatacontribution are even slimmer than in publishing, where there exists aculture of "publish or perish." This question must be resolved if wewish to enrich the context around datasets to make them more efficientlyconsumable. The future agenda is described in the concluding chapter by Paco Nathanand Ian MulvanyThe first step is to create a corpus of research publications to be usedfor training data during the Rich Context Competition.The next step will be a formal implementation of the knowledge graph,based primarily on extensions of open standards and use of open sourcesoftware. That graph is represented as an extension of a DCAT-compliantdata catalog. Immediate goals are to augment search and discovery insocial science research, plus additional use cases that help improve theknowledge graph and augment research.In the longer term, the process introduces human-in-the-loop AI intodata curation, ultimately to reward researchers and data stewards whosework contributes additional information into the system. With thislatter step, in the broader sense Rich Context helps establish acommunity focused on contributing code plus knowledge into the researchprocessReferences==========1\. C. F. Camerer *et al.*, Evaluating the replicability of socialscience experiments in Nature and Science between 2010 and 2015. *Nat.Hum. Behav.* **2**, 637 (2018).2. A. Dafoe, Science deserves better: theimperative to share complete replication files. *PS Polit. Sci. Polit.***47**, 60--66 (2014).3. J. P. A. Ioannidis, Why Most Published ResearchFindings Are False. *PLoS Med*. **2**, e124 (2005).4. N. Young, J.Ioannidis, O. Al-Ubaydli, Why Current Publication Practices May DistortScience. *PLoS Med* (2008).5. G. Christensen, E. Miguel, Transparency,reproducibility, and the credibility of economics research. *J. Econ.Lit.* **56**, 920--980 (2018).6. G. Colavizza, I. Hrynaszkiewicz, I.Staden, K. Whitaker, B. McGillivray, The citation advantage of linkingpublications to research data (2019), (available athttps://arxiv.org/pdf/1907.02565.pdf).7. T. Yarkoni *et al.*, "Enhancingand accelerating social science via automation: Challenges andOpportunities" (2019), , doi:10.31235/osf.io/vncwe.8. V. Bush, *Science,the endless frontier: A report to the President* (US Govt. print. off.,1945).9. L. Wissler, M. Almashraee, D. M. Díaz, A. Paschke, in *IEEEGSC* (2014).